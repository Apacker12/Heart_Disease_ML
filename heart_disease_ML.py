# -*- coding: utf-8 -*-
"""DSA507 Group Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FIsrGHCYiXArMJuvqqcY2AhIlYYFOP8x

#Team Project: End-to-End Machine Learning with Deployment

## Project Details

The purpose of this project is to give you hands-on experience with the full machine learning lifecycle, from problem definition to deployment. Working in teams, you will select a non-trivial dataset (either a real-world database or a complex open dataset) and design an ML system that can be deployed through a front-end interface such as Streamlit, Azure, AWS, or a comparable platform.

### Project Requirements

1. **Problem Framing**: Identify a meaningful applied problem that can be addressed using machine learning. Provide context and motivation for why the problem matters.
2. **Data Acquisition and Exploration**: Obtain the dataset (minimum 10,000 observations or equivalent complexity). Perform exploratory data analysis to understand structure, detect quality issues, and discover patterns.
3. **Data Preparation**: Apply preprocessing steps, including handling missing values, encodingcategorical variables, scaling, and feature engineering. Document your pipeline.
4. **Modeling**: Train and compare at least five machine learning models (linear models, tree-based methods, ensemble methods, or neural networks). Use cross-validation and hyperparameter tuning to optimize performance.
5. **Evaluation**: Assess models using appropriate metrics (classification, regression, or clustering metrics depending on your problem). Present confusion matrices, ROC curves, or regression error plots as applicable.
6. **Deployment**: Deploy your final model using a front-end interface. This could be a web app in Streamlit, a cloud service endpoint (such as Azure ML or AWS Sagemaker), or an equivalent environment that allows interaction with the model.
7. **Presentation**: Prepare a 5-minute video explaining your problem, dataset, preprocessing, models, results, and deployment. Highlight both the technical aspects and the practical implications of your solution.

### Project Deliverables

*   A documented Jupyter notebook with all code and results
*   A working deployed ML application accessible through a front-end interface
*   A final presentation video with the participation of all the team members.

##1) Problem Framing

Heart disease remains the leading cause of death for both men and women in the United states, where it's estimated that one person dies every 34 seconds from cardiovascular diseas according to CDC data. Proper diagnosis and treatment is paramount in treating the disease, where it's critical to understand features that may increase the risk of developing or having heart disease. Machine learning capabilities enable early and accurate insights into risk for heart disease. Therefore, the objective of this model is to accurately predict whether heart-disease exists in patients given various features.

https://www.cdc.gov/heart-disease/data-research/facts-stats/index.html#:~:text=Heart%20disease%20is%20the%20leading,lost%20productivity%20due%20to%20death.

##2) Data Acquisition & Exploration

### Data Acquisition

Using data from Kaggle located at https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset, with the following feature information:

**Features**:

*   **Age** | Objective Feature | age | int (days)
*   **Height** | Objective Feature | height | int (cm)
*   **Weight** | Objective Feature | weight | float (kg)
*   **Gender** | Objective Feature | gender | categorical code
*   **Systolic blood pressure** | Examination Feature | ap_hi | int
*   **Diastolic blood pressure** | Examination Feature | ap_lo | int
*   **Cholesterol** | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal
*   **Glucose** | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal
*   **Smoking** | Subjective Feature | smoke | binary
*  **Alcohol intake**| Subjective Feature | alco | binary
*   **Physical activity** | Subjective Feature | active | binary
*  **Presence or absence of cardiovascular disease** | Target Variable | cardio | binary
"""

# Importing packages
!pip install streamlit

# Importing libraries
import streamlit as st
import numpy as np
import joblib
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier

# Defining default font sizes
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

# To read the cardio_train.csv data
df_original = pd.read_csv('cardio_train.csv', sep=';')
df_original.head()

"""###Data Exploration"""

# Displaying the dataframe info
df_original.info()

# Viewing how many data fields have null values
df_original.isnull().sum()

"""There are zero null values, meaning there are no rows or columns needing to be removed from the data."""

# Pulling up statistical data on the heart disease dataset
df_original.describe()

"""The id information is unnecessary as it only identifies the line and will not be useful to the model. The age, we can see, is clearly not in years--something worth feature engineering. There are several features with drastic outliers reflected in the maximum amounts, compared to the mean and standard deviation information. Scaling should help, but it may be worth while to look into the ap_hi and ap_lo data which are the two biggest culprits of the drastic outliers.

##3) Data Preparation
"""

# Plotting the ap_hi data to get a better look at outliers
plt.figure(figsize=(12,5))
plt.scatter(df_original.index, df_original['ap_hi'], alpha=0.6, s=10)
plt.title("Scatterplot of ap_hi Values (Systolic BP)")
plt.xlabel("Row Index")
plt.ylabel("ap_hi")
plt.grid(True)
plt.show()

# Counting the number of rows where ap_hi is larger than 3000
df_original[df_original['ap_hi'] > 2000].shape[0]

"""There are roughly 9 outliers (of the 70K lines of data) for ap_hi."""

# Plotting the ap_lo data to get a better look at outliers
plt.figure(figsize=(12,5))
plt.scatter(df_original.index, df_original['ap_lo'], alpha=0.6, s=10)
plt.title("Scatterplot of ap_lo Values (Systolic BP)")
plt.xlabel("Row Index")
plt.ylabel("ap_lo")
plt.grid(True)
plt.show()

# Counting the number of rows where ap_lo is larger than 2000
df_original[df_original['ap_lo'] > 2000].shape[0]

"""There are only 25 ap_lo outliers, with a value greater than 2000."""

# Removing any rows where the ap_lo or ap_hi is greater than 2000
heart_df = df_original[(df_original['ap_lo'] <= 2000) & (df_original['ap_hi'] <= 2000)]

# Rechecking the statistical information on the dataset
heart_df.describe()

"""Although the outliers still remain, they are less drastic and should work better with scaling during our pipeline creation."""

# Creating a column where age is the number of years, rather than days
heart_df = heart_df.copy()
heart_df["age_years"] = (heart_df["age"] / 365.25).round()

# Creating BMI using height/weight
heart_df["bmi"] = (heart_df["weight"] / (heart_df["height"] / 100) ** 2).round(2)
heart_df.head()

"""### Correlation Analysis"""

# Creating a correlation matrix
corr_matrix = df_original.corr()

# Visualizing the correlation matrix
corr_matrix.style.background_gradient(cmap='viridis')

"""Looking at the target value 'cardio', there appears to be strong correlations that would be valuable to use in the machine learning model."""

# Grabbing correlations for cardiovascular disease
df_original.corr(numeric_only=True)["cardio"].drop("cardio").sort_values(ascending=False)*100

# Creating a correlation matrix
corr_matrix = heart_df.corr()

# Visualizing the correlation matrix
corr_matrix.style.background_gradient(cmap='viridis')

# Grabbing correlations for cardiovascular disease
heart_df.corr(numeric_only=True)["cardio"].drop("cardio").sort_values(ascending=False)*100

"""As expected, there are many strongly correlated features able to be used for the machine learning model where anything over 10 will be used. Although height isn't highly correlated, incorporating it into the BMI feature allows us to have an additional correlated feature for modeling. Age in years, on the other hand, doesn't appear to yield a stronger correlation. However, modifying to adjust for the outliers in the ap_hi and ap_lo columns shows great benefit. The ap_hi is now the highest correlated feature, with the ap_lo almost doubling in correlation. The model should have enough features for promising performance.

##4) Modeling

###Test/Train Set Creation
"""

# Separating the features from the target
X = heart_df.drop(columns=["cardio", "active", "smoke", "height", "alco", "id", "gender", "age_years"])
y = heart_df["cardio"].astype("int64")

# Separating the dataframe between training & test data
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""###4.1) Ridge Classifier"""

# Creating a pipeline
ridge_model = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", RidgeClassifier())
])

# Fitting the pipeline to training data
ridge_model.fit(x_train, y_train)

# Creating predictions on test data
ridge_y_pred = ridge_model.predict(x_test)

# Evaluating performance metrics
ridge_accuracy = accuracy_score(y_test, ridge_y_pred)*100
ridge_report = classification_report(y_test, ridge_y_pred)
ridge_conf_matrix = confusion_matrix(y_test, ridge_y_pred)
ridge_precision = precision_score(y_test, ridge_y_pred)
ridge_recall = recall_score(y_test, ridge_y_pred)
ridge_f1 = f1_score(y_test, ridge_y_pred)

# Displaying the confusion matrix & accuracy
print("Ridge Classifier (untuned) Confusion Matrix:\n", ridge_conf_matrix)
print(f"Ridge Classifier (untuned) Accuracy: {ridge_accuracy:.4f}%")

# Displaying the classification report
print("Ridge Classifier (untuned) Classification Report:\n", ridge_report)

"""The untuned Ridge model displays OK results, but they are not very accurate (~69%).

####Tuning the Ridge Hyperparameters
"""

# Cross-validation via gridsearch
ridge_params = {
    "ridge__alpha": [0.01, 0.1, 1.0, 10.0, 100.0]}
ridge_grid = GridSearchCV(
    estimator=ridge_model,
    param_grid=ridge_params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1)
ridge_grid.fit(x_train, y_train)

# Creating the best model for the
ridge_best_model = ridge_grid.best_estimator_

# Creating Predictions using test data
ridge_best_y_pred = ridge_best_model.predict(x_test)

# Computing model performance metrics
ridge_best_accuracy = accuracy_score(y_test, ridge_best_y_pred)*100
ridge_best_report = classification_report(y_test, ridge_best_y_pred)
ridge_best_conf_matrix = confusion_matrix(y_test, ridge_best_y_pred)
ridge_best_precision = precision_score(y_test, ridge_best_y_pred)
ridge_best_recall = recall_score(y_test, ridge_best_y_pred)
ridge_best_f1 = f1_score(y_test, ridge_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Ridge Classifier (tuned) Confusion Matrix:\n", ridge_best_conf_matrix)
print(f"Ridge Classifier (tuned) Accuracy: {ridge_best_accuracy:.4f}%")

# Displaying the classification report
print("Ridge Classifier (tuned) Classification Report:\n", ridge_best_report)

# Displaying the best parameters
print("Best Ridge parameters:")
print(ridge_grid.best_params_)

"""Hypertuning the Ridge model gives it a slight boost, but it remains under 70% accuracy. The best parameter for the Ridge alpha is 100.0. With this, the model may be adjusted to ensure this is the best parameter value."""

# Cross-validation via gridsearch
ridge_params = {
    "ridge__alpha": [100.0, 150.0, 175.0, 200.0, 250.0]}
ridge_grid = GridSearchCV(
    estimator=ridge_model,
    param_grid=ridge_params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1)
ridge_grid.fit(x_train, y_train)

# Creating the best model for the
ridge_best_model = ridge_grid.best_estimator_

# Creating Predictions using test data
ridge_best_y_pred = ridge_best_model.predict(x_test)

# Computing model performance metrics
ridge_best_accuracy = accuracy_score(y_test, ridge_best_y_pred)*100
ridge_best_report = classification_report(y_test, ridge_best_y_pred)
ridge_best_conf_matrix = confusion_matrix(y_test, ridge_best_y_pred)
ridge_best_precision = precision_score(y_test, ridge_best_y_pred)
ridge_best_recall = recall_score(y_test, ridge_best_y_pred)
ridge_best_f1 = f1_score(y_test, ridge_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Ridge Classifier (tuned) Confusion Matrix:\n", ridge_best_conf_matrix)
print(f"Ridge Classifier (tuned) Accuracy: {ridge_best_accuracy:.4f}%")

# Displaying the classification report
print("Ridge Classifier (tuned) Classification Report:\n", ridge_best_report)

# Displaying the best parameters
print("Best Ridge parameters:")
print(ridge_grid.best_params_)

"""Further inspection reveals the alpha for the Ridge Model should be closer to 150 to be optimal. Further tuning may provide an even better accuracy than 69.2368%."""

# Cross-validation via gridsearch
ridge_params = {
    "ridge__alpha": [145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 155.0,]}
ridge_grid = GridSearchCV(
    estimator=ridge_model,
    param_grid=ridge_params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1)
ridge_grid.fit(x_train, y_train)

# Creating the best model for the
ridge_best_model = ridge_grid.best_estimator_

# Creating Predictions using test data
ridge_best_y_pred = ridge_best_model.predict(x_test)

# Computing model performance metrics
ridge_best_accuracy = accuracy_score(y_test, ridge_best_y_pred)*100
ridge_best_report = classification_report(y_test, ridge_best_y_pred)
ridge_best_conf_matrix = confusion_matrix(y_test, ridge_best_y_pred)
ridge_best_precision = precision_score(y_test, ridge_best_y_pred)
ridge_best_recall = recall_score(y_test, ridge_best_y_pred)
ridge_best_f1 = f1_score(y_test, ridge_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Ridge Classifier (tuned) Confusion Matrix:\n", ridge_best_conf_matrix)
print(f"Ridge Classifier (tuned) Accuracy: {ridge_best_accuracy:.4f}%")

# Displaying the classification report
print("Ridge Classifier (tuned) Classification Report:\n", ridge_best_report)

# Displaying the best parameters
print("Best Ridge parameters:")
print(ridge_grid.best_params_)

"""The alpha at 148.0 is best for the Ridge model, and it doesn't break 70% accuracy. Possibly one of the other models is able to display better accuracy.

### 4.2) Decision Tree Classifier
"""

# Creating a pipeline
tree_model = Pipeline([
    ("scaler", StandardScaler()),
    ("tree", DecisionTreeClassifier(random_state=42))])

# Fitting the pipeline to training data
tree_model.fit(x_train, y_train)

# Creating predictions on the test data
tree_y_pred = tree_model.predict(x_test)

# Evaluating performance metrics
tree_accuracy = accuracy_score(y_test, tree_y_pred)*100
tree_report = classification_report(y_test, tree_y_pred)
tree_conf_matrix = confusion_matrix(y_test, tree_y_pred)
tree_precision = precision_score(y_test, tree_y_pred)
tree_recall = recall_score(y_test, tree_y_pred)
tree_f1 = f1_score(y_test, tree_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Tree Classifier (untuned) Confusion Matrix:\n", tree_conf_matrix)
print(f"Decision Tree Classifier (untuned) Accuracy: {tree_accuracy:.4f}%")

# Displaying the classification report
print("Decision Tree Classifier (untuned) Classification Report:\n", tree_report)

"""Well, it appears the untuned Decision tree doesn't look promising to be the model performing with over a 70% accuracy, but hypertuning is sure to improve performance.

####Tuning the Decision Tree Hyperparameters
"""

# Cross-validation via gridsearch
tree_params = {
    "tree__max_depth": [3, 5, 10, None],
    "tree__min_samples_split": [2, 5, 10],
    "tree__min_samples_leaf": [1, 2, 4]}

tree_grid = GridSearchCV(tree_model, tree_params, cv=5, scoring="accuracy")
tree_grid.fit(x_train, y_train)

# Creating the best model for the tree classifier
tree_best_model = tree_grid.best_estimator_

# Creating predictions on the test data
tree_best_y_pred = tree_best_model.predict(x_test)

# Evaluating performance metrics
tree_best_accuracy = accuracy_score(y_test, tree_best_y_pred)*100
tree_best_report = classification_report(y_test, tree_best_y_pred)
tree_best_conf_matrix = confusion_matrix(y_test, tree_best_y_pred)
tree_best_precision = precision_score(y_test, tree_best_y_pred)
tree_best_recall = recall_score(y_test, tree_best_y_pred)
tree_best_f1 = f1_score(y_test, tree_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Tree Classifier (tuned) Confusion Matrix:\n", tree_best_conf_matrix)
print(f"Decision Tree Classifier (tuned) Accuracy: {tree_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision Tree Classifier (tuned) Classification Report:\n", tree_best_report)

# Displaying the best parameters
print("Best Tree parameters:")
print(tree_grid.best_params_)

"""Hypertuning the Decision Tree broke the initial thought that it wouldn't be the model to perform at over a 70% accuracy. With the tuned model, the Decision Tree performs at almost 74% accuracy. With the nest parameters, the model may be adjusted to see if further improvements are possible."""

# Cross-validation via gridsearch
tree_params = {
    "tree__max_depth": [4, 5, 6],
    "tree__min_samples_split": [2, 3, 4],
    "tree__min_samples_leaf": [1, 2, 4]}

tree_grid = GridSearchCV(tree_model, tree_params, cv=5, scoring="accuracy")
tree_grid.fit(x_train, y_train)

# Creating the best model for the tree classifier
tree_best_model = tree_grid.best_estimator_

# Creating predictions on the test data
tree_best_y_pred = tree_best_model.predict(x_test)

# Evaluating performance metrics
tree_best_accuracy = accuracy_score(y_test, tree_best_y_pred)*100
tree_best_report = classification_report(y_test, tree_best_y_pred)
tree_best_conf_matrix = confusion_matrix(y_test, tree_best_y_pred)
tree_best_precision = precision_score(y_test, tree_best_y_pred)
tree_best_recall = recall_score(y_test, tree_best_y_pred)
tree_best_f1 = f1_score(y_test, tree_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Tree Classifier (tuned) Confusion Matrix:\n", tree_best_conf_matrix)
print(f"Decision Tree Classifier (tuned) Accuracy: {tree_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision Tree Classifier (tuned) Classification Report:\n", tree_best_report)

# Displaying the best parameters
print("Best Tree parameters:")
print(tree_grid.best_params_)

"""Okay, it turns out the model was optimal at 73.6316% accuracy. Now, to continue creation and evaluation of the other three models.

### 4.3) Random Forest Classifier
"""

# Creating a pipeline
forest_model = Pipeline([
    ("scaler", StandardScaler()),
    ("forest", RandomForestClassifier(random_state=42))])

# Fitting the pipeline to training data
forest_model.fit(x_train, y_train)

# Creating predictions on the test data
forest_y_pred = forest_model.predict(x_test)

# Evaluating performance metrics
forest_accuracy = accuracy_score(y_test, forest_y_pred)*100
forest_report = classification_report(y_test, forest_y_pred)
forest_conf_matrix = confusion_matrix(y_test, forest_y_pred)
forest_precision = precision_score(y_test, forest_y_pred)
forest_recall = recall_score(y_test, forest_y_pred)
forest_f1 = f1_score(y_test, forest_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Forest Classifier (untuned) Confusion Matrix:\n", forest_conf_matrix)
print(f"Decision Forest Classifier (untuned) Accuracy: {forest_accuracy:.4f}%")

# Displaying the classification report
print("Decision Forest Classifier (untuned) Classification Report:\n", forest_report)

"""Thus far, the Random Forest model has the best initial performance with 71% accuracy. Hypertuning the parameters may reveal a model performing above the Decision Tree's 73.6316% accuracy.

####Tuning the Random Forest Hyperparameters
"""

# Cross-validation via randomized search
forest_params = {
    "forest__n_estimators": [50, 100, 200],
    "forest__max_depth": [None, 5, 10, 15],
    "forest__min_samples_split": [2, 5, 10],
    "forest__min_samples_leaf": [1, 2, 4],
    "forest__max_features": ["sqrt", "log2", 0.5]}

forest_grid = RandomizedSearchCV(
    estimator=forest_model,
    param_distributions=forest_params,
    n_iter=10,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
forest_grid.fit(x_train, y_train)

# Creating the best model for the regressor
forest_best_model = forest_grid.best_estimator_

# Creating predictions on the test data
forest_best_y_pred = forest_best_model.predict(x_test)

# Evaluating performance metrics
forest_best_accuracy = accuracy_score(y_test, forest_best_y_pred)*100
forest_best_report = classification_report(y_test, forest_best_y_pred)
forest_best_conf_matrix = confusion_matrix(y_test, forest_best_y_pred)
forest_best_precision = precision_score(y_test, forest_best_y_pred)
forest_best_recall = recall_score(y_test, forest_best_y_pred)
forest_best_f1 = f1_score(y_test, forest_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Forest Classifier (tuned) Confusion Matrix:\n", forest_best_conf_matrix)
print(f"Decision Forest Classifier (tuned) Accuracy: {forest_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision Forest Classifier (tuned) Classification Report:\n", forest_best_report)

# Displaying the best parameters
print("Best Forest parameters:")
print(forest_grid.best_params_)

"""The first try, and the tuned hyperparameters of the Random Forest model improves performance above the Decision Tree's, which is now top-performing at 73.8102%. With new insights into the best values for these parameters, continuing to tune should prove beneficial unless these are the optimal assignments."""

# Cross-validation via randomized search
forest_params = {
    "forest__n_estimators": [173, 174, 175],
    "forest__max_depth": [10, 11, 12],
    "forest__min_samples_split": [7, 8, 9],
    "forest__min_samples_leaf": [1],
    "forest__max_features": ["log2"]}

forest_grid = RandomizedSearchCV(
    estimator=forest_model,
    param_distributions=forest_params,
    n_iter=10,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
forest_grid.fit(x_train, y_train)

# Creating the best model for the regressor
forest_best_model = forest_grid.best_estimator_

# Creating predictions on the test data
forest_best_y_pred = forest_best_model.predict(x_test)

# Evaluating performance metrics
forest_best_accuracy = accuracy_score(y_test, forest_best_y_pred)*100
forest_best_report = classification_report(y_test, forest_best_y_pred)
forest_best_conf_matrix = confusion_matrix(y_test, forest_best_y_pred)
forest_best_precision = precision_score(y_test, forest_best_y_pred)
forest_best_recall = recall_score(y_test, forest_best_y_pred)
forest_best_f1 = f1_score(y_test, forest_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision Forest Classifier (tuned) Confusion Matrix:\n", forest_best_conf_matrix)
print(f"Decision Forest Classifier (tuned) Accuracy: {forest_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision Forest Classifier (tuned) Classification Report:\n", forest_best_report)

# Displaying the best parameters
print("Best Forest parameters:")
print(forest_grid.best_params_)

"""Adjusting the parameters once more improves the accuracy slightly, to 73.8245%, and includes the best parameters

### 4.4) Logistic Regression
"""

# Creating a pipeline
LogReg_model = Pipeline([
    ("scaler", StandardScaler()),
    ("logreg", LogisticRegression(max_iter=1000, random_state=42))])

# Fitting the pipeline to training data
LogReg_model.fit(x_train, y_train)

# Creating predictions on the test data
LogReg_y_pred = LogReg_model.predict(x_test)

# Evaluating performance metrics
LogReg_accuracy = accuracy_score(y_test, LogReg_y_pred)*100
LogReg_report = classification_report(y_test, LogReg_y_pred)
LogReg_conf_matrix = confusion_matrix(y_test, LogReg_y_pred)
LogReg_precision = precision_score(y_test, LogReg_y_pred)
LogReg_recall = recall_score(y_test, LogReg_y_pred)
LogReg_f1 = f1_score(y_test, LogReg_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision LogReg (untuned) Confusion Matrix:\n", LogReg_conf_matrix)
print(f"Decision LogReg (untuned) Accuracy: {LogReg_accuracy:.4f}%")

# Displaying the classification report
print("Decision LogReg (untuned) Classification Report:\n", LogReg_report)

"""The Logistic Regression model also has a promising accuracy (72.5454%) as an untuned model. Tweaking the parameters should improve this performance.

####Tuning the Logistic Regression Hyperparameters
"""

# Cross-validation via randomized search
logreg_params = {
    "logreg__C": [0.01, 0.1, 1, 10],
    "logreg__penalty": ["l1", "l2"],
    "logreg__solver": ["liblinear"]}

logreg_grid = RandomizedSearchCV(
    estimator=LogReg_model,
    param_distributions=logreg_params,
    n_iter=5,
    cv=3,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
logreg_grid.fit(x_train, y_train)

# Creating the best model for the regressor
logreg_best_model = logreg_grid.best_estimator_

# Creating predictions on the test data
logreg_best_y_pred = logreg_best_model.predict(x_test)

# Evaluating performance metrics
LogReg_best_accuracy = accuracy_score(y_test, logreg_best_y_pred)*100
LogReg_best_report = classification_report(y_test, logreg_best_y_pred)
LogReg_best_conf_matrix = confusion_matrix(y_test, logreg_best_y_pred)
LogReg_best_precision = precision_score(y_test, logreg_best_y_pred)
LogReg_best_recall = recall_score(y_test, logreg_best_y_pred)
LogReg_best_f1 = f1_score(y_test, logreg_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision LogReg (tuned) Confusion Matrix:\n", LogReg_best_conf_matrix)
print(f"Decision LogReg (tuned) Accuracy: {LogReg_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision LogReg (tuned) Classification Report:\n", LogReg_best_report)

# Displaying the best parameters
print("Best LogReg parameters:")
print(logreg_grid.best_params_)

"""There's a slight improvement in performance. Further adjustments to the hyperparameter C may provide an additional boost."""

# Cross-validation via randomized search
logreg_params = {
    "logreg__C": [0.08, 0.09, 0.1, 0.11, 0.12],
    "logreg__penalty": ["l1"],
    "logreg__solver": ["liblinear"]}

logreg_grid = RandomizedSearchCV(
    estimator=LogReg_model,
    param_distributions=logreg_params,
    n_iter=5,
    cv=3,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
logreg_grid.fit(x_train, y_train)

# Creating the best model for the regressor
logreg_best_model = logreg_grid.best_estimator_

# Creating predictions on the test data
logreg_best_y_pred = logreg_best_model.predict(x_test)

# Evaluating performance metrics
LogReg_best_accuracy = accuracy_score(y_test, logreg_best_y_pred)*100
LogReg_best_report = classification_report(y_test, logreg_best_y_pred)
LogReg_best_conf_matrix = confusion_matrix(y_test, logreg_best_y_pred)
LogReg_best_precision = precision_score(y_test, logreg_best_y_pred)
LogReg_best_recall = recall_score(y_test, logreg_best_y_pred)
LogReg_best_f1 = f1_score(y_test, logreg_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision LogReg (tuned) Confusion Matrix:\n", LogReg_best_conf_matrix)
print(f"Decision LogReg (tuned) Accuracy: {LogReg_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision LogReg (tuned) Classification Report:\n", LogReg_best_report)

# Displaying the best parameters
print("Best LogReg parameters:")
print(logreg_grid.best_params_)

"""There is a slight adjustment in the best parameters assigned, but it results in the same performance--meaning this is the best model to use for the Logistic Regression with a 72.5597% accuracy.

### 4.5) k-Nearest Neighbors (k-NN) Classifier
"""

# Creating a pipeline
knn_model = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())])

# Fitting the pipeline to training data
knn_model.fit(x_train, y_train)

# Creating predictions on the test data
knn_y_pred = knn_model.predict(x_test)

# Evaluating performance metrics
knn_accuracy = accuracy_score(y_test, knn_y_pred)*100
knn_report = classification_report(y_test, knn_y_pred)
knn_conf_matrix = confusion_matrix(y_test, knn_y_pred)
knn_precision = precision_score(y_test, knn_y_pred)
knn_recall = recall_score(y_test, knn_y_pred)
knn_f1 = f1_score(y_test, knn_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision k-NN Classifier (untuned) Confusion Matrix:\n", knn_conf_matrix)
print(f"Decision k-NN Classifier (untuned) Accuracy: {knn_accuracy:.4f}%")

# Displaying the classification report
print("Decision k-NN Classifier (untuned) Classification Report:\n", knn_report)

"""The k-Nearest Neighbors has an average initial performance compared to the previous models. An adjustment in parameters may result in improvements to the ~70% accuracy

####Tuning the k-NN Hyperparameters
"""

# Cross-validation via randomized search
knn_params = {
    "knn__n_neighbors": [3, 5, 7, 10, 15],
    "knn__weights": ["uniform", "distance"],
    "knn__p": [1, 2]}

knn_grid = RandomizedSearchCV(
    estimator=knn_model,
    param_distributions=knn_params,
    n_iter=10,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
knn_grid.fit(x_train, y_train)

# Creating the best model for the regressor
knn_best_model = knn_grid.best_estimator_

# Creating predictions on the test data
knn_best_y_pred = knn_best_model.predict(x_test)

# Evaluating performance metrics
knn_best_accuracy = accuracy_score(y_test, knn_best_y_pred)*100
knn_best_report = classification_report(y_test, knn_best_y_pred)
knn_best_conf_matrix = confusion_matrix(y_test, knn_best_y_pred)
knn_best_precision = precision_score(y_test, knn_best_y_pred)
knn_best_recall = recall_score(y_test, knn_best_y_pred)
knn_best_f1 = f1_score(y_test, knn_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision k-NN Classifier (tuned) Confusion Matrix:\n", knn_best_conf_matrix)
print(f"Decision k-NN Classifier (tuned) Accuracy: {knn_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision k-NN Classifier (tuned) Classification Report:\n", knn_best_report)

# Displaying the best parameters
print("Best k-NN parameters:")
print(knn_grid.best_params_)

"""It performs two percentage points better for accuracy, meaning the k-Nearest Neighbors model may improve with additional tuning of parameters."""

# Cross-validation via randomized search
knn_params = {
    "knn__n_neighbors": [13, 14, 15, 16, 17],
    "knn__weights": ["uniform"],
    "knn__p": [1]}

knn_grid = RandomizedSearchCV(
    estimator=knn_model,
    param_distributions=knn_params,
    n_iter=10,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    random_state=42)
knn_grid.fit(x_train, y_train)

# Creating the best model for the regressor
knn_best_model = knn_grid.best_estimator_

# Creating predictions on the test data
knn_best_y_pred = knn_best_model.predict(x_test)

# Evaluating performance metrics
knn_best_accuracy = accuracy_score(y_test, knn_best_y_pred)*100
knn_best_report = classification_report(y_test, knn_best_y_pred)
knn_best_conf_matrix = confusion_matrix(y_test, knn_best_y_pred)
knn_best_precision = precision_score(y_test, knn_best_y_pred)
knn_best_recall = recall_score(y_test, knn_best_y_pred)
knn_best_f1 = f1_score(y_test, knn_best_y_pred)

# Displaying the confusion matrix & accuracy
print("Decision k-NN Classifier (tuned) Confusion Matrix:\n", knn_best_conf_matrix)
print(f"Decision k-NN Classifier (tuned) Accuracy: {knn_best_accuracy:.4f}%")

# Displaying the classification report
print("Decision k-NN Classifier (tuned) Classification Report:\n", knn_best_report)

# Displaying the best parameters
print("Best k-NN parameters:")
print(knn_grid.best_params_)

"""The best performing model for the k-Nearest Neighbors model reveals that 16 s the optimal number of  neighbors. Time to evaluate all of these hypertuned, final models.

##5) Evaluation
"""

# Building an evaluation
classification_summary = pd.DataFrame([
    {"Model": "Ridge (untuned)", "Accuracy": ridge_accuracy, "Precision": ridge_precision, "Recall": ridge_recall, "F1 Score": ridge_f1},
    {"Model": "Ridge (tuned)",   "Accuracy": ridge_best_accuracy, "Precision": ridge_best_precision, "Recall": ridge_best_recall, "F1 Score": ridge_best_f1},
    {"Model": "Decision Tree (untuned)", "Accuracy": tree_accuracy, "Precision": tree_precision, "Recall": tree_recall, "F1 Score": tree_f1},
    {"Model": "Decision Tree (tuned)",   "Accuracy": tree_best_accuracy, "Precision": tree_best_precision, "Recall": tree_best_recall, "F1 Score": tree_best_f1},
    {"Model": "Random Forest (untuned)", "Accuracy": forest_accuracy, "Precision": forest_precision, "Recall": forest_recall, "F1 Score": forest_f1},
    {"Model": "Random Forest (tuned)",   "Accuracy": forest_best_accuracy, "Precision": forest_best_precision, "Recall": forest_best_recall, "F1 Score": forest_best_f1},
    {"Model": "Logistic Regression (untuned)", "Accuracy": LogReg_accuracy, "Precision": LogReg_precision, "Recall": LogReg_recall, "F1 Score": LogReg_f1},
    {"Model": "Logistic Regression (tuned)",   "Accuracy": LogReg_best_accuracy, "Precision": LogReg_best_precision, "Recall": LogReg_best_recall, "F1 Score": LogReg_best_f1},
    {"Model": "k-NN (untuned)", "Accuracy": knn_accuracy, "Precision": knn_precision, "Recall": knn_recall, "F1 Score": knn_f1},
    {"Model": "k-NN (tuned)",   "Accuracy": knn_best_accuracy, "Precision": knn_best_precision, "Recall": knn_best_recall, "F1 Score": knn_best_f1}
])

# Round for readability
classification_summary[["Precision", "Recall", "F1 Score"]] = classification_summary[["Precision", "Recall", "F1 Score"]].round(4)

# Display the table
classification_summary.sort_values(by="Accuracy", ascending=False)

"""The decision tree comes out on top for all four metrics; accuracy, precision, recall, and F1 score. Therefore, that will be our final model."""

# Table for tuned models only
tuned_summary = pd.DataFrame([
    {"Model": "Ridge Model", "Accuracy": ridge_best_accuracy/100, "Precision": ridge_best_precision, "Recall": ridge_best_recall, "F1 Score": ridge_best_f1},
    {"Model": "Decision Tree Model", "Accuracy": tree_best_accuracy/100, "Precision": tree_best_precision, "Recall": tree_best_recall, "F1 Score": tree_best_f1},
    {"Model": "Random Forest Model", "Accuracy": forest_best_accuracy/100, "Precision": forest_best_precision, "Recall": forest_best_recall, "F1 Score": forest_best_f1},
    {"Model": "Logistic Regression Model", "Accuracy": LogReg_best_accuracy/100, "Precision": LogReg_best_precision, "Recall": LogReg_best_recall, "F1 Score": LogReg_best_f1},
    {"Model": "k-NN Model", "Accuracy": knn_best_accuracy/100, "Precision": knn_best_precision, "Recall": knn_best_recall, "F1 Score": knn_best_f1}
])

# Round for readability
tuned_summary[["Accuracy", "Precision", "Recall", "F1 Score"]] = tuned_summary[["Accuracy", "Precision", "Recall", "F1 Score"]].round(4)

tuned_summary.head()

# Transpose for plotting
tuned_summary_plot = tuned_summary.set_index("Model").T

# Plot each metric as a line
plt.style.use("fivethirtyeight")  # Alternatives: 'classic', 'fivethirtyeight', 'bmh'
plt.figure(figsize=(14, 8))
for metric in tuned_summary_plot.index:
    plt.plot(tuned_summary_plot.columns, tuned_summary_plot.loc[metric], marker="o", label=metric)

# Chart formatting
plt.title("Classification Metrics for Tuned Models")
plt.ylabel("Score")
plt.ylim(0.63,0.83)
plt.xticks(rotation=45, ha="right")
plt.grid(True, linestyle="--", alpha=0.7)
plt.legend(loc="upper right")
plt.tight_layout()
plt.show()

plt.style.use("fivethirtyeight")

# Preparing the data
metrics = ["Accuracy", "Precision", "Recall", "F1 Score"]
models = tuned_summary["Model"].tolist()
values = tuned_summary[metrics].values.T  # shape: (4 metrics, 5 models)

# Setting up bars
x = np.arange(len(models))
bar_width = 0.18
overlap_shift = 0.12

# Plotting each metric with slight overlap
plt.figure(figsize=(14, 10))
for i, metric in enumerate(metrics):
    plt.bar(x + i * overlap_shift, values[i], width=bar_width, label=metric)

# Formatting the plot
plt.xticks(x + overlap_shift * 1.5, models, rotation=45, ha="right")
plt.ylabel("Score")
plt.title("Classification Metrics for Tuned Models")
plt.ylim(.6, .82)
plt.legend(loc="upper right")
plt.grid(True, axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

"""Although the performance is generally pretty close across the five models, the visuals point to the Random Forest Model as being the best.

6) Deployment

Deployment is via Streamlit.
"""